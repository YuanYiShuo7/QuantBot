# Brainstorm
## Discussion
### Problem & Solution
#### 非深度思考模型
- Prefix Token 应该有什么？
- LoRA 先进行语义对齐训练，再进行强化学习。
#### 深度思考模型
- 思考模式结构化，由另一个强大的大模型评判思考模式关键token分数。
- 什么样的思考结构能最大地发挥模型的能力？
- 能不能绕过思考结构格式化，给关键token打分？
- 如果不能绕过，语义对齐训练怎么找优质数据？
- 微调语义对齐训练，数据来源可以是大模型已知结果生成原因，如：某某票为什么在某某天大涨，输入Prefix Token，让大模型输出原因，把原因作为Ground Truth，训练待微调大模型的推理过程。（因果互证）