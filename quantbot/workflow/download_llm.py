"""
ä¸‹è½½ LLM æ¨¡å‹åˆ°æœ¬åœ°ï¼ˆä½¿ç”¨ ModelScopeï¼‰
"""
import os
import sys
from pathlib import Path
import logging
import shutil

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

try:
    from modelscope import snapshot_download
    from transformers import AutoTokenizer, AutoModelForCausalLM
    import torch
except ImportError as e:
    logger.error(f"å¯¼å…¥ä¾èµ–å¤±è´¥: {e}")
    logger.info("è¯·å®‰è£…: pip install modelscope transformers torch")
    sys.exit(1)


def check_disk_space(required_gb: int = 20):
    """æ£€æŸ¥ç£ç›˜ç©ºé—´æ˜¯å¦è¶³å¤Ÿ"""
    try:
        total, used, free = shutil.disk_usage(".")
        free_gb = free // (2**30)
        logger.info(f"å½“å‰ç£ç›˜å¯ç”¨ç©ºé—´: {free_gb}GB")
        
        if free_gb < required_gb:
            logger.warning(f"ç£ç›˜ç©ºé—´å¯èƒ½ä¸è¶³ï¼éœ€è¦çº¦{required_gb}GBï¼Œå½“å‰ä»…æœ‰{free_gb}GB")
            return False
        return True
    except Exception as e:
        logger.warning(f"æ— æ³•æ£€æŸ¥ç£ç›˜ç©ºé—´: {str(e)}")
        return True


def download_model(model_name: str = 'Qwen/Qwen2-7B-Instruct', 
                   local_dir: str = None,
                   cache_dir: str = None):
    """
    ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹åˆ°æœ¬åœ°ç›®å½•
    
    Args:
        model_name: æ¨¡å‹åç§°
        local_dir: æœ¬åœ°ä¿å­˜ç›®å½•
        cache_dir: ç¼“å­˜ç›®å½•
    """
    try:
        # è®¾ç½®é»˜è®¤ç¼“å­˜ç›®å½•
        if cache_dir is None:
            cache_dir = "./quantbot/cache/model_cache"
        
        # è®¾ç½®ç¼“å­˜ç›®å½•
        cache_path = Path(cache_dir).resolve()
        cache_path.mkdir(parents=True, exist_ok=True)
        os.environ['MODELSCOPE_CACHE'] = str(cache_path)
        
        # ç¡®å®šæœ¬åœ°ä¿å­˜ç›®å½•
        if local_dir is None:
            model_dir_name = model_name.split('/')[-1]
            local_dir = Path("./quantbot/llm") / model_dir_name
        else:
            local_dir = Path(local_dir)
        
        local_dir = local_dir.resolve()
        local_dir.mkdir(parents=True, exist_ok=True)
        
        logger.info(f"å¼€å§‹ä¸‹è½½æ¨¡å‹: {model_name}")
        logger.info(f"ä¿å­˜ç›®å½•: {local_dir}")
        logger.info(f"ç¼“å­˜ç›®å½•: {cache_path}")
        
        # æ£€æŸ¥ç£ç›˜ç©ºé—´
        check_disk_space(20)
        
        # ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹
        logger.info("æ­£åœ¨ä¸‹è½½æ¨¡å‹ï¼ˆä½¿ç”¨ ModelScopeï¼‰...")
        
        downloaded_path = snapshot_download(
            model_id=model_name,
            cache_dir=str(cache_path),
            local_dir=str(local_dir),
            revision='master'
        )
        
        logger.info("âœ“ æ¨¡å‹ä¸‹è½½å®Œæˆ")
        
        # éªŒè¯æ¨¡å‹æ–‡ä»¶
        logger.info("éªŒè¯æ¨¡å‹æ–‡ä»¶...")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                str(local_dir),
                trust_remote_code=True
            )
            logger.info("âœ“ åˆ†è¯å™¨åŠ è½½æˆåŠŸ")
            
            model = AutoModelForCausalLM.from_pretrained(
                str(local_dir),
                trust_remote_code=True,
                torch_dtype=torch.float16,
                low_cpu_mem_usage=True
            )
            logger.info("âœ“ æ¨¡å‹åŠ è½½æˆåŠŸ")
            
            total_params = sum(p.numel() for p in model.parameters())
            logger.info(f"æ¨¡å‹å‚æ•°é‡: {total_params / 1e9:.1f}B")
            
        except Exception as e:
            logger.warning(f"æ¨¡å‹éªŒè¯è­¦å‘Š: {str(e)}")
        
        logger.info(f"ğŸ‰ æ¨¡å‹å·²æˆåŠŸä¸‹è½½åˆ°: {local_dir}")
        return str(local_dir)
        
    except Exception as e:
        logger.error(f"ä¸‹è½½æ¨¡å‹å¤±è´¥: {str(e)}")
        raise


def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(description='ä½¿ç”¨ ModelScope ä¸‹è½½æ¨¡å‹')
    parser.add_argument(
        '--model',
        type=str,
        default='Qwen/Qwen2-1.5B-Instruct',
        help='è¦ä¸‹è½½çš„æ¨¡å‹åç§°'
    )
    parser.add_argument(
        '--local-dir',
        type=str,
        default=None,
        help='æœ¬åœ°ä¿å­˜ç›®å½•ï¼ˆé»˜è®¤: ./quantbot/llm/{æ¨¡å‹å}ï¼‰'
    )
    parser.add_argument(
        '--cache-dir', 
        type=str,
        default=None,
        help='ç¼“å­˜ç›®å½•ï¼ˆé»˜è®¤: ./quantbot/cache/model_cacheï¼‰'
    )
    
    args = parser.parse_args()
    
    # æ„å»ºå®Œæ•´çš„ä¿å­˜è·¯å¾„
    if args.local_dir is None:
        model_dir_name = args.model.split('/')[-1]
        full_local_dir = f"./quantbot/llm/{model_dir_name}"
    else:
        full_local_dir = args.local_dir
    
    # æ„å»ºç¼“å­˜è·¯å¾„
    if args.cache_dir is None:
        full_cache_dir = "./quantbot/cache/model_cache"
    else:
        full_cache_dir = args.cache_dir
    
    print("=" * 50)
    print("ModelScope æ¨¡å‹ä¸‹è½½å·¥å…·")
    print("=" * 50)
    print(f"æ¨¡å‹: {args.model}")
    print(f"ä¿å­˜ç›®å½•: {full_local_dir}")
    print(f"ç¼“å­˜ç›®å½•: {full_cache_dir}")
    print("=" * 50)
    
    try:
        download_model(args.model, args.local_dir, args.cache_dir)
        print("\nğŸ‰ ä¸‹è½½å®Œæˆï¼")
    except Exception as e:
        print(f"\nâŒ ä¸‹è½½å¤±è´¥: {str(e)}")
        sys.exit(1)


if __name__ == '__main__':
    main()